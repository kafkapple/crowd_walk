{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Oct 15 16:23:00 2018\n",
    "\n",
    "@author: 2014_Joon_IBS\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "from keras.models import load_model\n",
    "import cv2\n",
    "import os\n",
    "import glob \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))   \n",
    "\n",
    "# 폴더/ 파일 이름을 숫자 순서대로 접근해서, 각 data 를 model 에 넣고, 그 prediction 값을 순차적으로 저장\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import itertools  # for confusion matrix plot\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize = False,\n",
    "                          title = 'Confusion matrix',\n",
    "                          cmap = plt.cm.Blues):  \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
    "        print(\"normalized\")\n",
    "    else:\n",
    "        print('without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap = cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment = \"center\",\n",
    "                 color = \"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def make_confusion_matrix(model, x, y, normalize = True):\n",
    "    predicted = model.predict(x)\n",
    "\n",
    "    pred_list = []; actual_list = []\n",
    "    for i in predicted:\n",
    "        pred_list.append(np.argmax(i))\n",
    "    for i in y:\n",
    "        actual_list.append(np.argmax(i))\n",
    "\n",
    "    confusion_result = confusion_matrix(actual_list, pred_list)\n",
    "    plot_confusion_matrix(confusion_result, classes = class_label, normalize = normalize, title = 'Confusion_matrix')\n",
    "    return confusion_result\n",
    "\n",
    "def plot_hist(hist):\n",
    "    plt.figure(0)\n",
    "    fig, loss_ax = plt.subplots()\n",
    "    acc_ax = loss_ax.twinx()\n",
    "\n",
    "    loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "    loss_ax.set_xlabel('epoch')\n",
    "    loss_ax.set_ylabel('loss')\n",
    "    loss_ax.legend(loc='upper left')\n",
    "\n",
    "    acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "    acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "    acc_ax.set_ylabel('accuracy')\n",
    "    acc_ax.legend(loc='lower left')\n",
    "\n",
    "    plt.show()\n",
    "    #plt.savefig('loss_accuracy_plot')\n",
    "def preprocess_img(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  #load img as grayscale\n",
    "    img = clahe.apply(img)  # histogram equalization\n",
    "    img = np.array(img)/255.  # normalize\n",
    "    img = img.reshape(-1, 40, 40,1) # to make input dimension same as model\n",
    "    return img\n",
    "\n",
    "\n",
    "# for list save\n",
    "def csv_write(result, name): # \n",
    "    with open(\"output_{}.csv\".format(name),'wb') as resultFile:\n",
    "        wr = csv.writer(resultFile, dialect='excel')\n",
    "        wr.writerows(result)\n",
    "\n",
    "def pred_from_file(model, data_path):\n",
    "    result, list_dir, list_files = pred_from_img_path(model, data_path)\n",
    "    \n",
    "    np_result = np.array(result) # convert into numpy array\n",
    "    np_result = np.squeeze(np_result, axis=1)\n",
    "    \n",
    "    #np.savetxt(\"mice_result.csv\", np_result, delimiter=\",\")\n",
    "    \n",
    "    print(np.shape(np_result)) # result shape check\n",
    "    \n",
    "    list_dir = pd.DataFrame(list_dir)\n",
    "    list_files = pd.DataFrame(list_files)\n",
    "    \n",
    "    list_dir.to_csv('dir_result.csv')\n",
    "    list_files.to_csv('files_result.csv')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================\n",
      "loading datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1006, 1115, 1337, 1338, 1115, 1225]\n",
      "4566\n",
      "1428\n",
      "1142\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#class_label = ['angry', 'happy','neutral']\n",
    "#class_label = ['angry', 'disgust', 'fear','happy','sad','surprise','neutral']\n",
    "#class_label = ['background', 'body','nose', 'tail']\n",
    "class_label = ['1', '2', '3', '4', '5', '6']\n",
    "n_class = len(class_label)\n",
    "fig_size = 96\n",
    "# load data\n",
    "print(\"===============================================================\")\n",
    "print(\"loading datasets\")\n",
    "path = r'F:\\Data\\gen\\crowd_aug'\n",
    "os.chdir(path)\n",
    "\n",
    "model = load_model('ak_crowd_96.h5')\n",
    "\n",
    "x_data = np.load('./x.npy')\n",
    "y_data = np.load('./y.npy')\n",
    "\n",
    "x_data = x_data.reshape(-1, fig_size,fig_size,1)\n",
    "y_data = np.argmax(y_data, axis=1) # convert one hot encoding to catogorical integer\n",
    "    # 2. arrange the data. shape change, use specific class only, ...\n",
    "\n",
    "\n",
    "class_dist = [len(y_data[y_data==i]) for i, c in enumerate(class_label)] \n",
    "print(class_dist)\n",
    "\n",
    "y_data = np_utils.to_categorical(y_data, n_class)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2, shuffle = True, random_state=33)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2, shuffle = True, random_state=33)\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(x_test))\n",
    "print(len(x_val))\n",
    "\n",
    "# prepare compile for fit (learning / testing)\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8),metrics=['accuracy'])\n",
    "\n",
    "# early stopping to prevent overfitting.\n",
    "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0.001, patience=10, verbose=1, mode='max')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4566 samples, validate on 1142 samples\n",
      "Epoch 1/10\n",
      "4566/4566 [==============================] - 1204s 264ms/step - loss: 1.6071 - acc: 0.4549 - val_loss: 0.9251 - val_acc: 0.6480\n",
      "Epoch 2/10\n",
      "4566/4566 [==============================] - 1203s 263ms/step - loss: 0.7910 - acc: 0.7028 - val_loss: 0.5523 - val_acc: 0.8109\n",
      "Epoch 3/10\n",
      "1344/4566 [=======>......................] - ETA: 14:28 - loss: 0.5234 - acc: 0.8065"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "hist = model.fit(x_train, y_train, #validation_split=0.2,\n",
    "                      validation_data = (x_val, y_val), \n",
    "                      shuffle = True, \n",
    "                      batch_size = 32, epochs = epoch, verbose = 1, \n",
    "                      callbacks = [early_stopping] )\n",
    "scores = model.evaluate(x_test, y_test, batch_size = 32)    \n",
    "\n",
    "\n",
    "# evaluate the trained model / weight using test data (not training data)\n",
    "scores = model.evaluate(x_test, y_test, batch_size=32)      \n",
    "confusion_result = make_confusion_matrix(model, x_test, y_test, True)\n",
    "print(scores)\n",
    "# This is for saving the model as png file. \n",
    "#test_fig = plot_model(model, to_file='BaseNet.png', show_shapes=True, show_layer_names=True)\n",
    "#plt.plot(test_fig)       \n",
    "plot_hist(hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
